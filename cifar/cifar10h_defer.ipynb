{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "caP7MazGRC45"
   },
   "source": [
    "# Load CIFAR-10H data\n",
    "This notebook will run our experiments using an expert based on the CIFAR-10H dataset.\n",
    "\n",
    "Results are for the Confidence approach and our method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "GLocp-PoTKsU",
    "outputId": "476c6b91-c5f6-4662-c892-5c6b50bd424a"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from https://github.com/jcpeterson/cifar-10h/blob/master/data/cifar10h-probs.npy, copied in repo for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUVt6VjARAXR"
   },
   "outputs": [],
   "source": [
    "\n",
    "cifar10h = np.load('cifar10h-probs.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332,
     "referenced_widgets": [
      "f457bd8605bb4865bc1e6fff15fb45d4",
      "bf61a80759c14cfc84c6d0e7dd95fdb8",
      "231e2c2f3ad544dcbb4e418292905e44",
      "fc0fbaac1c894552bfe68c6b875616dc",
      "e46b7d4de79946b293e3314589cf8e9f",
      "ffcfa5b2b079444d92d4ff4d389a5f2e",
      "3045f75b464f473e8396b12f952039ea",
      "6d272b15248f4257a64ec8b65bf70c51"
     ]
    },
    "colab_type": "code",
    "id": "W76vtWtuRtk-",
    "outputId": "acd7a73d-2588-4689-bdc0-2ee146b7e13f"
   },
   "outputs": [],
   "source": [
    "def metrics_cifar10h(cifar10h, loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    j = 0\n",
    "    class_wise = [0] * 10\n",
    "    class_counts = [0] * 10\n",
    "    for data in loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        batch_size = labels.size()[0]            # batch_size\n",
    "        for i in range(0,batch_size):\n",
    "            exp_prediction = np.argmax(np.random.multinomial(1, cifar10h[j]))\n",
    "            total += 1\n",
    "            j+= 1\n",
    "            correct += (exp_prediction == labels[i]).item()\n",
    "            class_wise[labels[i].item()] += (exp_prediction == labels[i]).item()\n",
    "            class_counts[labels[i].item()] += 1\n",
    "    for i in range(0,10):\n",
    "        class_wise[i] = 100*class_wise[i] / class_counts[i]\n",
    "    to_print={\"classifier accuracy\":100*correct/total }\n",
    "    return 100*correct/total, class_wise\n",
    "\n",
    "\n",
    "def metrics_cifar10h_class_wise(cifar10h, loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    j = 0\n",
    "    class_wise = [0] * 10\n",
    "    class_counts = [0] * 10\n",
    "    for data in loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        batch_size = labels.size()[0]            # batch_size\n",
    "        for i in range(0,batch_size):\n",
    "            exp_prediction = np.argmax(np.random.multinomial(1, cifar10h[j]))\n",
    "            total += 1\n",
    "            j+= 1\n",
    "            correct += (exp_prediction == labels[i]).item()\n",
    "            class_wise[labels[i].item()] += (exp_prediction == labels[i]).item()\n",
    "            class_counts[labels[i].item()] += 1\n",
    "    for i in range(0,10):\n",
    "        class_wise[i] = 100*class_wise[i] / class_counts[i]\n",
    "    return class_wise\n",
    "\n",
    "# data\n",
    "normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                     std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "if False:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: F.pad(x.unsqueeze(0),\n",
    "                            (4,4,4,4),mode='reflect').squeeze()),\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "        ])\n",
    "else:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "        ])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    ])\n",
    "\n",
    "dataset = 'cifar10'\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.__dict__[dataset.upper()]('../data', train=True, download=True,\n",
    "                        transform=transform_train),\n",
    "    batch_size=128, shuffle=True, **kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.__dict__[dataset.upper()]('../data', train=False, transform=transform_test),\n",
    "    batch_size=128, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print class wise accuracies of expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vS3URzcYN0aS",
    "outputId": "2043dd80-b097-4443-d595-522c19e4c609"
   },
   "outputs": [],
   "source": [
    "total, class_wise = metrics_cifar10h(cifar10h,val_loader)\n",
    "print(f'average accuracy of CIFAR-10H expert is: {total}')\n",
    "print(f'accuracy on the 10 classes is: {class_wise}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GH2OPcCUtkG"
   },
   "source": [
    "# Load other utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GADYd1csUvUj"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
    "                               padding=0, bias=False) or None\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
    "        layers = []\n",
    "        for i in range(int(nb_layers)):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
    "        assert((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) / 6\n",
    "        block = BasicBlock\n",
    "        # 1st conv before any network block\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "        self.softmax = nn.Softmax()\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06Kp63GxUwqG"
   },
   "outputs": [],
   "source": [
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "    \n",
    "import random\n",
    "def metrics_print(net, expert_fn, n_classes, loader):\n",
    "    correct = 0\n",
    "    correct_sys = 0\n",
    "    exp = 0\n",
    "    exp_total = 0\n",
    "    total = 0\n",
    "    real_total = 0\n",
    "    alone_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            batch_size = outputs.size()[0]            # batch_size\n",
    "            exp_prediction = expert_fn(images, labels)\n",
    "            for i in range(0,batch_size):\n",
    "                r = (predicted[i].item() == n_classes)\n",
    "                prediction = predicted[i]\n",
    "                if predicted[i] == 10:\n",
    "                    max_idx = 0\n",
    "                    # get second max\n",
    "                    for j in range(0,10):\n",
    "                        if outputs.data[i][j] >= outputs.data[i][max_idx]:\n",
    "                            max_idx = j\n",
    "                    prediction = max_idx\n",
    "                else:\n",
    "                    prediction = predicted[i]\n",
    "                alone_correct += (prediction ==labels[i]).item()\n",
    "                if r==0:\n",
    "                    total += 1\n",
    "                    correct += (predicted[i] == labels[i]).item()\n",
    "                    correct_sys += (predicted[i] == labels[i]).item()\n",
    "                if r==1:\n",
    "                    exp += (exp_prediction[i] == labels[i].item())\n",
    "                    correct_sys +=(exp_prediction[i] == labels[i].item())\n",
    "                    exp_total+=1\n",
    "                real_total += 1\n",
    "    cov = str(total) + str(\" out of\") + str(real_total)\n",
    "    to_print={\"coverage\":cov, \"system accuracy\": 100*correct_sys/real_total, \"expert accuracy\":100* exp/(exp_total+0.0002),\"classifier accuracy\":100*correct/(total+0.0001), \"alone classifier\": 100*alone_correct/real_total }\n",
    "    print(to_print)\n",
    "\n",
    "def metrics_print_baseline(net_class,   expert_fn, n_classes, loader):\n",
    "    correct = 0\n",
    "    correct_sys = 0\n",
    "    exp = 0\n",
    "    exp_total = 0\n",
    "    total = 0\n",
    "    real_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs_class = net_class(images)\n",
    "            _, predicted = torch.max(outputs_class.data, 1)\n",
    "            batch_size = outputs_class.size()[0]            # batch_size\n",
    "            \n",
    "            exp_prediction = expert_fn(images, labels)\n",
    "            for i in range(0,batch_size):\n",
    "                r = (exp_prediction[i] == labels[i].item())\n",
    "                if r==0:\n",
    "                    total += 1\n",
    "                    prediction = predicted[i]\n",
    "                    if predicted[i] == 10:\n",
    "                        max_idx = 0\n",
    "                        for j in range(0,10):\n",
    "                            if outputs_class.data[i][j] >= outputs_class.data[i][max_idx]:\n",
    "                                max_idx = j\n",
    "                        prediction = max_idx\n",
    "                    else:\n",
    "                        prediction = predicted[i]\n",
    "                    correct += (prediction == labels[i]).item()\n",
    "                    correct_sys += (prediction == labels[i]).item()\n",
    "                if r==1:\n",
    "                    exp += (exp_prediction[i] == labels[i].item())\n",
    "                    correct_sys +=(exp_prediction[i] == labels[i].item())\n",
    "                    exp_total+=1\n",
    "                real_total += 1\n",
    "    cov = str(total) + str(\" out of\") + str(real_total)\n",
    "    to_print={\"coverage\":cov, \"system accuracy\": 100*correct_sys/real_total, \"expert accuracy\":100* exp/(exp_total+0.0002),\"classifier accuracy\":100*correct/(total+0.0001) }\n",
    "    print(to_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EKVbm-77U2X-"
   },
   "source": [
    "# Baseline: Confidence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a  trained classification model on CIFAR-10. This model can be obtained from the cifar10_defer_baselines notebook. For convenience we have provided a trained model in ./models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tYju8LdcVZG1"
   },
   "outputs": [],
   "source": [
    "model_classifier = torch.load(\"./models/model_base\")\n",
    "model_classifier.eval()\n",
    "model_classifier.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPeVrStjWrTO"
   },
   "outputs": [],
   "source": [
    "def my_CrossEntropyLoss(outputs, labels):\n",
    "    batch_size = outputs.size()[0]            # batch_size\n",
    "    outputs =  - torch.log2(outputs[range(batch_size), labels]+0.00001)   # pick the values corresponding to the labels\n",
    "    return torch.sum(outputs)/batch_size\n",
    "\n",
    "def train_expert(train_loader_exp, model, optimizer, scheduler, epoch, expert_fn, n_classes):\n",
    "    \"\"\"Train for one epoch on the training set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader_exp):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        # compute new target\n",
    "        batch_size = output.size()[0]            # batch_size\n",
    "        m = [0] * batch_size\n",
    "        for j in range (0,batch_size):\n",
    "            m[j] = dataset_expert[str(input[j].cpu().numpy())]\n",
    "        m = torch.tensor(m)\n",
    "        m = m.to(device)\n",
    "        # compute loss\n",
    "        loss = my_CrossEntropyLoss(output, m)\n",
    "\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, m, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      loss=losses, top1=top1))\n",
    "\n",
    "\n",
    "def validate_expert(val_loader_exp, model, epoch, expert_fn, n_classes):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader_exp):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "        # expert prediction\n",
    "        batch_size = output.size()[0]            # batch_size\n",
    "        m = [0] * batch_size\n",
    "        for j in range (0,batch_size):\n",
    "            m[j] = dataset_expert[str(input[j].cpu().numpy())]\n",
    "        m = torch.tensor(m)\n",
    "        m = m.to(device)\n",
    "        # compute loss\n",
    "        loss = my_CrossEntropyLoss(output, m)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, m, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                      top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "    return top1.avg\n",
    "best_prec1 = 0\n",
    "def run_expert(model, data_aug, n_dataset, expert_fn, epochs):\n",
    "    global best_prec1\n",
    "    # Data loading code\n",
    "    \n",
    "    # get the number of model parameters\n",
    "    print('Number of model parameters: {}'.format(\n",
    "        sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "    # for training on multiple GPUs.\n",
    "    # Use CUDA_VISIBLE_DEVICES=0,1 to specify which GPUs to use\n",
    "    # model = torch.nn.DataParallel(model).cuda()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    \n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 0.1,\n",
    "                                momentum=0.9, nesterov = True,\n",
    "                                weight_decay=5e-4)\n",
    "\n",
    "    # cosine learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader)*200)\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        # train for one epoch\n",
    "        train_expert(val_loader, model, optimizer, scheduler, epoch, expert_fn, n_dataset)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate_expert(val_loader_rej, model, epoch, expert_fn, n_dataset)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "    print('Best accuracy: ', best_prec1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E64rJkcQiGGl"
   },
   "outputs": [],
   "source": [
    "def metrics_print_confid_cifar10h(net_mod, net_exp, dataset_expert_probs, n_classes, loader):\n",
    "    correct = 0\n",
    "    correct_sys = 0\n",
    "    exp = 0\n",
    "    exp_total = 0\n",
    "    total = 0\n",
    "    real_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs_mod = net_mod(images)\n",
    "            outputs_exp = net_exp(images)\n",
    "            _, predicted = torch.max(outputs_mod.data, 1)\n",
    "            _, predicted_exp = torch.max(outputs_exp.data, 1)\n",
    "            batch_size = outputs_mod.size()[0]            # batch_size\n",
    "            for i in range(0,batch_size):\n",
    "                r_score = 1 - outputs_mod.data[i][predicted[i].item()].item()\n",
    "                r_score = r_score - outputs_exp.data[i][1].item()\n",
    "                r = 0\n",
    "                if r_score >= 0:\n",
    "                    r = 1\n",
    "                else:\n",
    "                    r =  0\n",
    "                if r==0:\n",
    "                    total += 1\n",
    "                    correct += (predicted[i] == labels[i]).item()\n",
    "                    correct_sys += (predicted[i] == labels[i]).item()\n",
    "                if r==1:\n",
    "                    exp_prediction = np.argmax(np.random.multinomial(1, dataset_expert_probs[str(images[i].cpu().numpy())]))\n",
    "                    exp += (exp_prediction == labels[i].item())\n",
    "                    correct_sys +=(exp_prediction == labels[i].item())\n",
    "                    exp_total+=1\n",
    "                real_total += 1\n",
    "    cov = str(total) + str(\" out of\") + str(real_total)\n",
    "    to_print={\"coverage\":cov, \"system accuracy\": 100*correct_sys/real_total, \"expert accuracy\":100* exp/(exp_total+0.0002),\"classifier accuracy\":100*correct/(total+0.0001) }\n",
    "    print(to_print)\n",
    "    return [100*total/real_total,  100*correct_sys/real_total, 100* exp/(exp_total+0.0002),100*correct/(total+0.0001) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run experiment for confidence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hG1gtto_W2RD",
    "outputId": "54cc6b15-a3ae-4ae1-cbed-b94908abf190"
   },
   "outputs": [],
   "source": [
    "experiment_data = []\n",
    "max_trials = 10\n",
    "for experiment in range(0, max_trials):\n",
    "    normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                        std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "        ])\n",
    "\n",
    "    n_dataset =10\n",
    "    dataset = 'cifar10'\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.__dict__[dataset.upper()]('../data', train=False, transform=transform_test),\n",
    "        batch_size=128, shuffle=False, **kwargs)\n",
    "\n",
    "    dataset_expert = {}\n",
    "    dataset_expert_probs = {}\n",
    "    j = 0\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        batch_size = labels.size()[0]            # batch_size\n",
    "        for i in range(0,batch_size):\n",
    "            exp_prediction = np.argmax(np.random.multinomial(1, cifar10h[j]))\n",
    "            if (exp_prediction == labels[i]).item() :\n",
    "                dataset_expert[str(images[i].cpu().numpy())] =  0\n",
    "            else:\n",
    "                dataset_expert[str(images[i].cpu().numpy())] = 1\n",
    "            dataset_expert_probs[str(images[i].cpu().numpy())] = cifar10h[j]\n",
    "            j+= 1\n",
    "    normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                     std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "        ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "        ])\n",
    "\n",
    "    n_dataset = 10\n",
    "    dataset = 'cifar10'\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.__dict__[dataset.upper()]('../data', train=True, download=True,\n",
    "                                        transform=transform_train),\n",
    "                        batch_size=128, shuffle=True, **kwargs)\n",
    "\n",
    "    val_dataset_all = datasets.__dict__[dataset.upper()]('../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    val_size = int(0.5 * len(val_dataset_all))\n",
    "    val_size_rej = len(val_dataset_all) - val_size\n",
    "\n",
    "    val_dataset, val_dataset_rej = torch.utils.data.random_split(val_dataset_all, [val_size, val_size_rej])\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=128, shuffle=True, **kwargs)\n",
    "\n",
    "    val_loader_rej = torch.utils.data.DataLoader(\n",
    "        val_dataset_rej,\n",
    "        batch_size=128, shuffle=True, **kwargs)\n",
    "\n",
    "    model_expert = WideResNet(28, 2, 4, dropRate=0)\n",
    "    run_expert(model_expert, False, n_dataset, 5, 1)\n",
    "    batch_data = metrics_print_confid_cifar10h(model_classifier, model_expert, dataset_expert_probs, 10, val_loader_rej)\n",
    "    experiment_data.append(batch_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "UUxcCrF57TQz",
    "outputId": "809a0880-828e-48a6-c6d3-8614a55150db"
   },
   "outputs": [],
   "source": [
    "metrics = ['coverage', 'system accuracy', 'expert accuracy', 'classifier accuracy']\n",
    "print(\"Results for confidence approach\")\n",
    "for i in range(0,4):\n",
    "    arr = [0] * max_trials\n",
    "    for j in range(0,max_trials):\n",
    "        arr[j] = experiment_data[j][i]\n",
    "    print(f'{metrics[i]}: avg = {np.average(arr):.3f}, std= {np.std(arr):.3f}  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jc1wJ8MYv_9d"
   },
   "source": [
    "# Our method: 2-stage training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8IKRDjVgwku0"
   },
   "outputs": [],
   "source": [
    "def metrics_print_my_cifar10h(net_mod, dataset_expert_probs, n_classes, loader):\n",
    "    correct = 0\n",
    "    correct_sys = 0\n",
    "    exp = 0\n",
    "    exp_total = 0\n",
    "    total = 0\n",
    "    real_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs_mod = net_mod(images)\n",
    "            _, predicted = torch.max(outputs_mod.data, 1)\n",
    "            batch_size = outputs_mod.size()[0]            # batch_size\n",
    "            for i in range(0,batch_size):\n",
    "                r = (predicted[i] == 10)\n",
    "                if r==0:\n",
    "                    total += 1\n",
    "                    correct += (predicted[i] == labels[i]).item()\n",
    "                    correct_sys += (predicted[i] == labels[i]).item()\n",
    "                if r==1:\n",
    "                    exp_prediction = np.argmax(np.random.multinomial(1, dataset_expert_probs[str(images[i].cpu().numpy())]))\n",
    "                    exp += (exp_prediction == labels[i].item())\n",
    "                    correct_sys +=(exp_prediction == labels[i].item())\n",
    "                    exp_total+=1\n",
    "                real_total += 1\n",
    "    cov = str(total) + str(\" out of\") + str(real_total)\n",
    "    to_print={\"coverage\":cov, \"system accuracy\": 100*correct_sys/real_total, \"expert accuracy\":100* exp/(exp_total+0.0002),\"classifier accuracy\":100*correct/(total+0.0001) }\n",
    "    print(to_print)\n",
    "    return [100*total/real_total,  100*correct_sys/real_total, 100* exp/(exp_total+0.0002),100*correct/(total+0.0001) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NqGazyYwCS0"
   },
   "outputs": [],
   "source": [
    "def reject_CrossEntropyLoss(outputs, m, labels, m2, n_classes):\n",
    "    '''\n",
    "    The L_{CE} loss implementation for CIFAR\n",
    "    ----\n",
    "    outputs: network outputs\n",
    "    m: cost of deferring to expert cost of classifier predicting (I_{m =y})\n",
    "    labels: target\n",
    "    m2:  cost of classifier predicting (alpha* I_{m\\neq y} + I_{m =y})\n",
    "    n_classes: number of classes\n",
    "    '''\n",
    "    batch_size = outputs.size()[0]  # batch_size\n",
    "    rc = [n_classes] * batch_size\n",
    "    outputs = -m * torch.log2(outputs[range(batch_size), rc]) - m2 * torch.log2(\n",
    "        outputs[range(batch_size), labels])  \n",
    "    return torch.sum(outputs) / batch_size\n",
    "\n",
    "def train_reject(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_classes, alpha):\n",
    "    \"\"\"Train for one epoch on the training set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        c = 0 #1-cost\n",
    "        # expert  predictions\n",
    "        batch_size = output.size()[0]            # batch_size\n",
    "        m = [0] * batch_size\n",
    "        m2 = [0] * batch_size\n",
    "        for j in range (0,batch_size):\n",
    "            if str(input[j].cpu().numpy()) in dataset_expert:\n",
    "                exp = dataset_expert[str(input[j].cpu().numpy())]\n",
    "                if not exp:\n",
    "                    m[j] =  1\n",
    "                    m2[j] = alpha\n",
    "                else:\n",
    "                    m[j] = 0\n",
    "                    m2[j] = 1\n",
    "            else:\n",
    "                m[j] = 0\n",
    "                m2[j] = 1\n",
    "        m = torch.tensor(m)\n",
    "        m2 = torch.tensor(m2)\n",
    "        m = m.to(device)\n",
    "        m2 = m2.to(device)\n",
    "\n",
    "        # compute loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        #loss = criterion(output,target)\n",
    "        loss = reject_CrossEntropyLoss(output, m, target, m2, n_classes)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(2,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      loss=losses, top1=top1))\n",
    "\n",
    "\n",
    "def validate_reject(val_loader, model, epoch, expert_fn, n_classes):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "        # expert prediction\n",
    "        batch_size = output.size()[0]            # batch_size\n",
    "        m = [0] * batch_size\n",
    "        alpha = 0.5\n",
    "        m2 = [0] * batch_size\n",
    "        for j in range (0,batch_size):\n",
    "            if str(input[j].cpu().numpy()) in dataset_expert:\n",
    "                exp = dataset_expert[str(input[j].cpu().numpy())]\n",
    "                if not exp:\n",
    "                    m[j] =  1\n",
    "                    m2[j] = alpha\n",
    "                else:\n",
    "                    m[j] = 0\n",
    "                    m2[j] = 1\n",
    "            else:\n",
    "                m[j] = 0\n",
    "                m2[j] = 1\n",
    "        m = torch.tensor(m)\n",
    "        m2 = torch.tensor(m2)\n",
    "        m = m.to(device)\n",
    "        m2 = m2.to(device)\n",
    "        # compute loss\n",
    "        loss = reject_CrossEntropyLoss(output, m, target, m2, n_classes)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(2,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                      top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "    return top1.avg\n",
    "best_prec1 = 0\n",
    "def run_reject(model, data_aug, n_dataset, expert_fn, epochs, alpha):\n",
    "    global best_prec1\n",
    "\n",
    "    # get the number of model parameters\n",
    "    print('Number of model parameters: {}'.format(\n",
    "        sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "    # for training on multiple GPUs.\n",
    "    # Use CUDA_VISIBLE_DEVICES=0,1 to specify which GPUs to use\n",
    "    # model = torch.nn.DataParallel(model).cuda()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    \n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 0.0001,\n",
    "                                momentum=0.9, nesterov = True,\n",
    "                                weight_decay=5e-4)\n",
    "\n",
    "    # cosine learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader)*200)\n",
    "\n",
    "    for epoch in range(0, 1):\n",
    "        # train for one epoch\n",
    "        train_reject(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_dataset, alpha)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate_reject(val_loader_rej, model, epoch, expert_fn, n_dataset)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        if epoch % 1 == 0:\n",
    "            metrics_print_my_cifar10h(model, dataset_expert_probs, n_dataset, val_loader_rej)\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "        # train for one epoch\n",
    "        train_reject(val_loader, model, optimizer, scheduler, epoch, expert_fn, n_dataset, alpha)\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            metrics_print_my_cifar10h(model, dataset_expert_probs, n_dataset, val_loader_rej)\n",
    "    print('Best accuracy: ', best_prec1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWtr7aXm6-nI"
   },
   "source": [
    "# Our method: impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ajd_i4i6Ai-2"
   },
   "source": [
    "## train model of expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "69sG2_cvAtLz"
   },
   "outputs": [],
   "source": [
    "def my_CrossEntropyLoss(outputs, labels):\n",
    "    # m: expert costs, labels: ground truth, n_classes: number of classes\n",
    "    batch_size = outputs.size()[0]            # batch_size\n",
    "    outputs =  - torch.log2(outputs[range(batch_size), labels]+0.0001)   # pick the values corresponding to the labels\n",
    "    return torch.sum(outputs)/batch_size\n",
    "\n",
    "def train_expert(train_loader_exp, model, optimizer, scheduler, epoch, expert_fn, n_classes):\n",
    "    \"\"\"Train for one epoch on the training set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader_exp):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        # compute new target\n",
    "        batch_size = output.size()[0]            # batch_size\n",
    "        m = [0] * batch_size\n",
    "        for j in range (0,batch_size):\n",
    "            m[j] = dataset_expert[str(input[j].cpu().numpy())]\n",
    "        m = torch.tensor(m)\n",
    "        m = m.to(device)\n",
    "        # compute loss\n",
    "        loss = my_CrossEntropyLoss(output, m)\n",
    "\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, m, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      loss=losses, top1=top1))\n",
    "\n",
    "\n",
    "def validate_expert(val_loader_exp, model, epoch, expert_fn, n_classes):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader_exp):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "        # expert prediction\n",
    "        batch_size = output.size()[0]            # batch_size\n",
    "        m = [0] * batch_size\n",
    "        for j in range (0,batch_size):\n",
    "            m[j] = dataset_expert[str(input[j].cpu().numpy())]\n",
    "\n",
    "        m = torch.tensor(m)\n",
    "        m = m.to(device)\n",
    "        # compute loss\n",
    "        loss = my_CrossEntropyLoss(output, m)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, m, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                      top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "    return top1.avg\n",
    "best_prec1 = 0\n",
    "def run_expert(model, data_aug, n_dataset, expert_fn, epochs):\n",
    "    global best_prec1\n",
    "    # Data loading code\n",
    "    \n",
    "    # get the number of model parameters\n",
    "    print('Number of model parameters: {}'.format(\n",
    "        sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "    # for training on multiple GPUs.\n",
    "    # Use CUDA_VISIBLE_DEVICES=0,1 to specify which GPUs to use\n",
    "    # model = torch.nn.DataParallel(model).cuda()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    \n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 0.1,\n",
    "                                momentum=0.9, nesterov = True,\n",
    "                                weight_decay=5e-4)\n",
    "\n",
    "    # cosine learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader)*200)\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        # train for one epoch\n",
    "        train_expert(val_loader, model, optimizer, scheduler, epoch, expert_fn, n_dataset)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate_expert(val_loader_rej, model, epoch, expert_fn, n_dataset)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "    print('Best accuracy: ', best_prec1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4jx-EcHEwz9"
   },
   "source": [
    "augment dataset with predictions from expert network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmCxMi6RFMmY"
   },
   "outputs": [],
   "source": [
    "def reject_CrossEntropyLoss(outputs, m, labels, m2, n_classes):\n",
    "    # m: expert costs, labels: ground truth, n_classes: number of classes\n",
    "    batch_size = outputs.size()[0]            # batch_size\n",
    "    rc = [n_classes] * batch_size\n",
    "    rc = torch.tensor(rc)\n",
    "    #outputs =  - torch.log2(outputs[range(batch_size), labels])   # regular CE\n",
    "    outputs =  -m*torch.log2( outputs[range(batch_size), rc]) - m2*torch.log2(outputs[range(batch_size), labels])   # pick the values corresponding to the labels\n",
    "    return torch.sum(outputs)/batch_size\n",
    "def train_reject_(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_classes, alpha):\n",
    "    \"\"\"Train for one epoch on the training set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        c = 0 #1-cost\n",
    "        # expert  predictions\n",
    "        batch_size = output.size()[0]            # batch_size\n",
    "        m = [0] * batch_size\n",
    "        m2 = [1] * batch_size\n",
    "        m = torch.tensor(m)\n",
    "        m2 = torch.tensor(m2)\n",
    "        m = m.to(device)\n",
    "        m2 = m2.to(device)\n",
    "\n",
    "        # compute loss\n",
    "        #loss = criterion(output,target)\n",
    "        loss = reject_CrossEntropyLoss(output, m, target, m2, n_classes)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      loss=losses, top1=top1))\n",
    "\n",
    "def train_reject_impute(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_classes, alpha):\n",
    "    \"\"\"Train for one epoch on the training set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        c = 0 #1-cost\n",
    "        # expert  predictions\n",
    "        batch_size = output.size()[0]            # batch_size\n",
    "        m = [0] * batch_size\n",
    "        m2 = [0] * batch_size\n",
    "        for j in range (0,batch_size):\n",
    "            if str(input[j].cpu().numpy()) in dataset_expert:\n",
    "                exp = dataset_expert[str(input[j].cpu().numpy())]\n",
    "                if not exp:\n",
    "                    m[j] =  1\n",
    "                    m2[j] = alpha\n",
    "                else:\n",
    "                    m[j] = 0\n",
    "                    m2[j] = 1\n",
    "            else:\n",
    "                m[j] = 0\n",
    "                m2[j] = 1\n",
    "        m = torch.tensor(m)\n",
    "        m2 = torch.tensor(m2)\n",
    "        m = m.to(device)\n",
    "        m2 = m2.to(device)\n",
    "\n",
    "        # compute loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        #loss = criterion(output,target)\n",
    "        loss = reject_CrossEntropyLoss(output, m, target, m2, n_classes)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      loss=losses, top1=top1))\n",
    "\n",
    "\n",
    "def validate_reject_impute(val_loader, model, epoch, expert_fn, n_classes):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "        # expert prediction\n",
    "        batch_size = output.size()[0]            # batch_size\n",
    "        m = [0] * batch_size\n",
    "        alpha = 0.5\n",
    "        m2 = [0] * batch_size\n",
    "        for j in range (0,batch_size):\n",
    "            if str(input[j].cpu().numpy()) in dataset_expert:\n",
    "                exp = dataset_expert[str(input[j].cpu().numpy())]\n",
    "                if not exp:\n",
    "                    m[j] =  1\n",
    "                    m2[j] = alpha\n",
    "                else:\n",
    "                    m[j] = 0\n",
    "                    m2[j] = 1\n",
    "            else:\n",
    "                m[j] = 0\n",
    "                m2[j] = 1\n",
    "        m = torch.tensor(m)\n",
    "        m2 = torch.tensor(m2)\n",
    "        m = m.to(device)\n",
    "        m2 = m2.to(device)\n",
    "        # compute loss\n",
    "        loss = reject_CrossEntropyLoss(output, m, target, m2, n_classes)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                      top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "    return top1.avg\n",
    "best_prec1 = 0\n",
    "def run_reject_impute(model, data_aug, n_dataset, expert_fn, epochs, alpha):\n",
    "    global best_prec1\n",
    "\n",
    "    # get the number of model parameters\n",
    "    print('Number of model parameters: {}'.format(\n",
    "        sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "    # for training on multiple GPUs.\n",
    "    # Use CUDA_VISIBLE_DEVICES=0,1 to specify which GPUs to use\n",
    "    # model = torch.nn.DataParallel(model).cuda()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    \n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 0.0001,\n",
    "                                momentum=0.9, nesterov = True,\n",
    "                                weight_decay=5e-4)\n",
    "\n",
    "    # cosine learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader)*200)\n",
    "\n",
    "    for epoch in range(0, 10):\n",
    "        # train for one epoch\n",
    "        train_reject_impute(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_dataset, alpha)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate_reject_impute(val_loader_rej, model, epoch, expert_fn, n_dataset)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        if epoch % 1 == 0:\n",
    "            metrics_print_my_cifar10h(model, dataset_expert_probs, n_dataset, val_loader_rej)\n",
    "    \n",
    "    for epoch in range(0, 2):\n",
    "        # train for one epoch\n",
    "        train_reject_impute(val_loader, model, optimizer, scheduler, epoch, expert_fn, n_dataset, alpha)\n",
    "    \n",
    "        # evaluate on validation set\n",
    "        prec1 = validate_reject_impute(val_loader_rej, model, epoch, expert_fn, n_dataset)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        if epoch % 1 == 0:\n",
    "            metrics_print_my_cifar10h(model, dataset_expert_probs, n_dataset, val_loader_rej)\n",
    "    print('Best accuracy: ', best_prec1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run experiment for our method impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NDvIZ_5g6cLh",
    "outputId": "48d8a487-3a33-46f1-d704-155e91e83c34"
   },
   "outputs": [],
   "source": [
    "experiment_data = []\n",
    "max_trials = 10\n",
    "for experiment in range(0, max_trials):\n",
    "    normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                        std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "        ])\n",
    "\n",
    "    n_dataset =10\n",
    "    dataset = 'cifar10'\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.__dict__[dataset.upper()]('../data', train=False, transform=transform_test),\n",
    "        batch_size=128, shuffle=False, **kwargs)\n",
    "\n",
    "    dataset_expert = {}\n",
    "    dataset_expert_probs = {}\n",
    "    j = 0\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        batch_size = labels.size()[0]            # batch_size\n",
    "        for i in range(0,batch_size):\n",
    "            exp_prediction = np.argmax(np.random.multinomial(1, cifar10h[j]))\n",
    "            if (exp_prediction == labels[i]).item() :\n",
    "                dataset_expert[str(images[i].cpu().numpy())] =  0\n",
    "            else:\n",
    "                dataset_expert[str(images[i].cpu().numpy())] = 1\n",
    "            dataset_expert_probs[str(images[i].cpu().numpy())] = cifar10h[j]\n",
    "            j+= 1\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                        std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "        ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "        ])\n",
    "\n",
    "    n_dataset = 10\n",
    "    dataset = 'cifar10'\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.__dict__[dataset.upper()]('../data', train=True, download=True,\n",
    "                                        transform=transform_train),\n",
    "                        batch_size=128, shuffle=True, **kwargs)\n",
    "\n",
    "    val_dataset_all = datasets.__dict__[dataset.upper()]('../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    val_size = int(0.5 * len(val_dataset_all))\n",
    "    val_size_rej = len(val_dataset_all) - val_size\n",
    "\n",
    "    val_dataset, val_dataset_rej = torch.utils.data.random_split(val_dataset_all, [val_size, val_size_rej])\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=128, shuffle=True, **kwargs)\n",
    "\n",
    "    val_loader_rej = torch.utils.data.DataLoader(\n",
    "        val_dataset_rej,\n",
    "        batch_size=128, shuffle=True, **kwargs)\n",
    "\n",
    "    combined_data = torch.utils.data.ConcatDataset([datasets.__dict__[dataset.upper()]('../data', train=True, download=True,\n",
    "                                        transform=transform_train),val_dataset])\n",
    "    train_loader = torch.utils.data.DataLoader(combined_data,\n",
    "                        batch_size=128, shuffle=True, **kwargs)\n",
    "\n",
    "    model_expert = WideResNet(28, 2, 4, dropRate=0)\n",
    "    run_expert(model_expert, False, n_dataset, 2, 2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            for data in train_loader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs_mod = model_expert(images)\n",
    "                _, predicted = torch.max(outputs_mod.data, 1)\n",
    "                batch_size = outputs_mod.size()[0]            # batch_size\n",
    "                for i in range(0,batch_size):\n",
    "                    exp_prediction = predicted[i]\n",
    "                    dataset_expert[str(images[i].cpu().numpy())] =  exp_prediction\n",
    "    alpha = 1\n",
    "    model = torch.load(\"models/model_rejector_base\")\n",
    "\n",
    "    run_reject_impute(model, False, n_dataset, 2, 15, alpha)\n",
    "    batch_data =  metrics_print_my_cifar10h(model, dataset_expert_probs, n_dataset, val_loader_rej)\n",
    "    experiment_data.append(batch_data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['coverage', 'system accuracy', 'expert accuracy', 'classifier accuracy']\n",
    "print(\"Results for our method Impute \")\n",
    "for i in range(0,4):\n",
    "    arr = [0] * max_trials\n",
    "    for j in range(0,max_trials):\n",
    "        arr[j] = experiment_data[j][i]\n",
    "    print(f'{metrics[i]}: avg = {np.average(arr):.3f}, std= {np.std(arr):.3f}  ')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "EKVbm-77U2X-",
    "Jc1wJ8MYv_9d"
   ],
   "name": "CIFAR-10H.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "231e2c2f3ad544dcbb4e418292905e44": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffcfa5b2b079444d92d4ff4d389a5f2e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e46b7d4de79946b293e3314589cf8e9f",
      "value": 1
     }
    },
    "3045f75b464f473e8396b12f952039ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d272b15248f4257a64ec8b65bf70c51": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf61a80759c14cfc84c6d0e7dd95fdb8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e46b7d4de79946b293e3314589cf8e9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f457bd8605bb4865bc1e6fff15fb45d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_231e2c2f3ad544dcbb4e418292905e44",
       "IPY_MODEL_fc0fbaac1c894552bfe68c6b875616dc"
      ],
      "layout": "IPY_MODEL_bf61a80759c14cfc84c6d0e7dd95fdb8"
     }
    },
    "fc0fbaac1c894552bfe68c6b875616dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d272b15248f4257a64ec8b65bf70c51",
      "placeholder": "​",
      "style": "IPY_MODEL_3045f75b464f473e8396b12f952039ea",
      "value": "170500096it [00:02, 72790713.49it/s]"
     }
    },
    "ffcfa5b2b079444d92d4ff4d389a5f2e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
