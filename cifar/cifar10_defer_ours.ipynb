{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes through learning a classifier and rejector using our proposed loss function L_{CE}^{\\alpha} for a synthehtic expert (described in the paper) on CIFAR-10/100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a7Aympk-qwGu"
   },
   "source": [
    "# Preliminaries: model definition and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "cFhWBUzvqmZF",
    "outputId": "33d8f1b7-e93c-4bbf-8a78-e6f45961c087"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "plcXrgr8q_dR"
   },
   "source": [
    "WideResNet from following [repo](https://github.com/xternalz/WideResNet-pytorch/blob/master/wideresnet.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qP2NJY4BrEH8"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
    "                                                                padding=0, bias=False) or None\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
    "\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
    "        layers = []\n",
    "        for i in range(int(nb_layers)):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
    "        assert ((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) / 6\n",
    "        block = BasicBlock\n",
    "        # 1st conv before any network block\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "        self.softmax = nn.Softmax()\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ll6QD_IrQeD"
   },
   "source": [
    "metrics and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m93DvFTXrRwt"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def metrics_print(net,expert_fn, n_classes, loader):\n",
    "    '''\n",
    "    Computes metrics for deferal\n",
    "    -----\n",
    "    Arguments:\n",
    "    net: model\n",
    "    expert_fn: expert model\n",
    "    n_classes: number of classes\n",
    "    loader: data loader\n",
    "    '''\n",
    "    correct = 0\n",
    "    correct_sys = 0\n",
    "    exp = 0\n",
    "    exp_total = 0\n",
    "    total = 0\n",
    "    real_total = 0\n",
    "    alone_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            batch_size = outputs.size()[0]  # batch_size\n",
    "            exp_prediction = expert_fn(images, labels)\n",
    "            for i in range(0, batch_size):\n",
    "                r = (predicted[i].item() == n_classes)\n",
    "                prediction = predicted[i]\n",
    "                if predicted[i] == n_classes:\n",
    "                    max_idx = 0\n",
    "                    # get second max\n",
    "                    for j in range(0, n_classes):\n",
    "                        if outputs.data[i][j] >= outputs.data[i][max_idx]:\n",
    "                            max_idx = j\n",
    "                    prediction = max_idx\n",
    "                else:\n",
    "                    prediction = predicted[i]\n",
    "                alone_correct += (prediction == labels[i]).item()\n",
    "                if r == 0:\n",
    "                    total += 1\n",
    "                    correct += (predicted[i] == labels[i]).item()\n",
    "                    correct_sys += (predicted[i] == labels[i]).item()\n",
    "                if r == 1:\n",
    "                    exp += (exp_prediction[i] == labels[i].item())\n",
    "                    correct_sys += (exp_prediction[i] == labels[i].item())\n",
    "                    exp_total += 1\n",
    "                real_total += 1\n",
    "    cov = str(total) + str(\" out of\") + str(real_total)\n",
    "    to_print = {\"coverage\": cov, \"system accuracy\": 100 * correct_sys / real_total,\n",
    "                \"expert accuracy\": 100 * exp / (exp_total + 0.0002),\n",
    "                \"classifier accuracy\": 100 * correct / (total + 0.0001),\n",
    "                \"alone classifier\": 100 * alone_correct / real_total}\n",
    "    print(to_print)\n",
    "\n",
    "def metrics_print_baseline(net_class, expert_fn, n_classes, loader):\n",
    "    correct = 0\n",
    "    correct_sys = 0\n",
    "    exp = 0\n",
    "    exp_total = 0\n",
    "    total = 0\n",
    "    real_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs_class = net_class(images)\n",
    "            _, predicted = torch.max(outputs_class.data, 1)\n",
    "            batch_size = outputs_class.size()[0]  # batch_size\n",
    "\n",
    "            exp_prediction = expert_fn(images, labels)\n",
    "            for i in range(0, batch_size):\n",
    "                r = (exp_prediction[i] == labels[i].item())\n",
    "                if r == 0:\n",
    "                    total += 1\n",
    "                    prediction = predicted[i]\n",
    "                    if predicted[i] == n_classes:\n",
    "                        max_idx = 0\n",
    "                        for j in range(0, n_classes):\n",
    "                            if outputs_class.data[i][j] >= outputs_class.data[i][max_idx]:\n",
    "                                max_idx = j\n",
    "                        prediction = max_idx\n",
    "                    else:\n",
    "                        prediction = predicted[i]\n",
    "                    correct += (prediction == labels[i]).item()\n",
    "                    correct_sys += (prediction == labels[i]).item()\n",
    "                if r == 1:\n",
    "                    exp += (exp_prediction[i] == labels[i].item())\n",
    "                    correct_sys += (exp_prediction[i] == labels[i].item())\n",
    "                    exp_total += 1\n",
    "                real_total += 1\n",
    "    cov = str(total) + str(\" out of\") + str(real_total)\n",
    "    to_print = {\"coverage\": cov, \"system accuracy\": 100 * correct_sys / real_total,\n",
    "                \"expert accuracy\": 100 * exp / (exp_total + 0.0002),\n",
    "                \"classifier accuracy\": 100 * correct / (total + 0.0001)}\n",
    "    print(to_print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-GzRg6nrsSB"
   },
   "source": [
    "# Our Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4bojqcOrsdDA"
   },
   "outputs": [],
   "source": [
    "def reject_CrossEntropyLoss(outputs, m, labels, m2, n_classes):\n",
    "    '''\n",
    "    The L_{CE} loss implementation for CIFAR\n",
    "    ----\n",
    "    outputs: network outputs\n",
    "    m: cost of deferring to expert cost of classifier predicting (alpha* I_{m\\neq y} + I_{m =y})\n",
    "    labels: target\n",
    "    m2:  cost of classifier predicting (alpha* I_{m\\neq y} + I_{m =y})\n",
    "    n_classes: number of classes\n",
    "    '''\n",
    "    batch_size = outputs.size()[0]  # batch_size\n",
    "    rc = [n_classes] * batch_size\n",
    "    outputs = -m * torch.log2(outputs[range(batch_size), rc]) - m2 * torch.log2(\n",
    "        outputs[range(batch_size), labels])  \n",
    "    return torch.sum(outputs) / batch_size\n",
    "\n",
    "def my_CrossEntropyLoss(outputs, labels):\n",
    "    # Regular Cross entropy loss\n",
    "    batch_size = outputs.size()[0]  # batch_size\n",
    "    outputs = - torch.log2(outputs[range(batch_size), labels])  # regular CE\n",
    "    return torch.sum(outputs) / batch_size\n",
    "\n",
    "def train_reject(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_classes, alpha):\n",
    "    \"\"\"Train for one epoch on the training set with deferral\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "\n",
    "        # get expert  predictions and costs\n",
    "        batch_size = output.size()[0]  # batch_size\n",
    "        m = expert_fn(input, target)\n",
    "        m2 = [0] * batch_size\n",
    "        for j in range(0, batch_size):\n",
    "            if m[j] == target[j].item():\n",
    "                m[j] = 1\n",
    "                m2[j] = alpha\n",
    "            else:\n",
    "                m[j] = 0\n",
    "                m2[j] = 1\n",
    "        m = torch.tensor(m)\n",
    "        m2 = torch.tensor(m2)\n",
    "        m = m.to(device)\n",
    "        m2 = m2.to(device)\n",
    "        # done getting expert predictions and costs \n",
    "        # compute loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = reject_CrossEntropyLoss(output, m, target, m2, n_classes)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                loss=losses, top1=top1))\n",
    "\n",
    "def train_reject_class(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_classes, alpha):\n",
    "    \"\"\"Train for one epoch on the training set without deferral\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "\n",
    "        # compute loss\n",
    "        loss = my_CrossEntropyLoss(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                loss=losses, top1=top1))\n",
    "\n",
    "\n",
    "def validate_reject(val_loader, model, epoch, expert_fn, n_classes):\n",
    "    \"\"\"Perform validation on the validation set with deferral\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "        # expert prediction\n",
    "        batch_size = output.size()[0]  # batch_size\n",
    "        m = expert_fn(input, target)\n",
    "        alpha = 1\n",
    "        m2 = [0] * batch_size\n",
    "        for j in range(0, batch_size):\n",
    "            if m[j] == target[j].item():\n",
    "                m[j] = 1\n",
    "                m2[j] = alpha\n",
    "            else:\n",
    "                m[j] = 0\n",
    "                m2[j] = 1\n",
    "        m = torch.tensor(m)\n",
    "        m2 = torch.tensor(m2)\n",
    "        m = m.to(device)\n",
    "        m2 = m2.to(device)\n",
    "        # compute loss\n",
    "        loss = reject_CrossEntropyLoss(output, m, target, m2, n_classes)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "    return top1.avg\n",
    "def run_reject(model, data_aug, n_dataset, expert_fn, epochs, alpha):\n",
    "    '''\n",
    "    model: WideResNet model\n",
    "    data_aug: boolean to use data augmentation in training\n",
    "    n_dataset: number of classes\n",
    "    expert_fn: expert model\n",
    "    epochs: number of epochs to train\n",
    "    alpha: alpha parameter in L_{CE}^{\\alpha}\n",
    "    '''\n",
    "    # Data loading code\n",
    "    normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                     std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "    if data_aug:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: F.pad(x.unsqueeze(0),\n",
    "                                              (4, 4, 4, 4), mode='reflect').squeeze()),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomCrop(32),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    else:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    if n_dataset == 10:\n",
    "        dataset = 'cifar10'\n",
    "    elif n_dataset == 100:\n",
    "        dataset = 'cifar100'\n",
    "\n",
    "    kwargs = {'num_workers': 0, 'pin_memory': True}\n",
    "\n",
    "\n",
    "    train_dataset_all = datasets.__dict__[dataset.upper()]('../data', train=True, download=True,\n",
    "                                                           transform=transform_train)\n",
    "    train_size = int(0.90 * len(train_dataset_all))\n",
    "    test_size = len(train_dataset_all) - train_size\n",
    "\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(train_dataset_all, [train_size, test_size])\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=128, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                               batch_size=128, shuffle=True, **kwargs)\n",
    "    # get the number of model parameters\n",
    "    print('Number of model parameters: {}'.format(\n",
    "        sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "    # for training on multiple GPUs.\n",
    "    # Use CUDA_VISIBLE_DEVICES=0,1 to specify which GPUs to use\n",
    "    # model = torch.nn.DataParallel(model).cuda()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 0.1,\n",
    "                                momentum=0.9, nesterov=True,\n",
    "                                weight_decay=5e-4)\n",
    "\n",
    "    # cosine learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader) * epochs)\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        # train for one epoch\n",
    "        train_reject(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_dataset, alpha)\n",
    "        if epoch % 10 == 0:\n",
    "            metrics_print(model, expert_fn, n_dataset, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqf_r99A1OcX"
   },
   "source": [
    "# Initialize expert and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZxCEtfhRAFB"
   },
   "outputs": [],
   "source": [
    "k = 5 # number of classes expert can predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qXfR2kpyzs_5"
   },
   "outputs": [],
   "source": [
    "n_dataset = 10  # cifar-10\n",
    "\n",
    "class synth_expert:\n",
    "    '''\n",
    "    simple class to describe our synthetic expert on CIFAR-10\n",
    "    ----\n",
    "    k: number of classes expert can predict\n",
    "    n_classes: number of classes (10+1 for CIFAR-10)\n",
    "    '''\n",
    "    def __init__(self, k, n_classes):\n",
    "        self.k = k\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def predict(self, input, labels):\n",
    "        batch_size = labels.size()[0]  # batch_size\n",
    "        outs = [0] * batch_size\n",
    "        for i in range(0, batch_size):\n",
    "            if labels[i].item() <= self.k:\n",
    "                outs[i] = labels[i].item()\n",
    "            else:\n",
    "                prediction_rand = random.randint(0, self.n_classes - 1)\n",
    "                outs[i] = prediction_rand\n",
    "        return outs\n",
    "\n",
    "\n",
    "expert = synth_expert(k, n_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5PwaJBFx2dFA"
   },
   "outputs": [],
   "source": [
    "model = WideResNet(28, n_dataset + 1, 4, dropRate=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "38ccdcaff71c4e10be16761856e700be",
      "84bb659b50644494b25209d5df54f495",
      "2702db773dda485487e0443e69c33aa4",
      "5581a11b53e54b2fab584cd3f277ac03",
      "11c0877d8d8743898f68d6ccb7bcdf8c",
      "4aed2120237b48a099d2a8fdc6121949",
      "70d729f1145a4b2d850d0f6f9643f4fc",
      "099a0a0fb2c34f578a460402c4a9695f"
     ]
    },
    "colab_type": "code",
    "id": "hyZFRl-hBinm",
    "outputId": "353dd74b-19ee-4863-d7a0-8ae1d3a04ad8"
   },
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "run_reject(model, True, n_dataset, expert.predict, 200, alpha) # train for 200 epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation set\n",
    "This produces the key metrics measured:\n",
    "- coverage: percentage of examples where classifier predicts\n",
    "- system accuracy: accuracy of combined system on all data (with deferral)\n",
    "- classifier accuracy: accuracy of classifier on non-deferred examples\n",
    "- expert accuracy: accuracy of expert on deferred examples\n",
    "- classifier alone accuracy: accuracy of classifier on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "HoXOkAJ-TBlh",
    "outputId": "b6deb738-6fce-4214-958b-e70a19278d49"
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                 std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.__dict__[\"cifar100\".upper()]('../data', train=False, transform=transform_test, download=True),\n",
    "    batch_size=128, shuffle=True, **kwargs)\n",
    "\n",
    "metrics_print(model, expert.predict, n_dataset, val_loader)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "g_J2uOpzEhvb"
   ],
   "name": "CIFAR0-10 learning to reject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "099a0a0fb2c34f578a460402c4a9695f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11c0877d8d8743898f68d6ccb7bcdf8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2702db773dda485487e0443e69c33aa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4aed2120237b48a099d2a8fdc6121949",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_11c0877d8d8743898f68d6ccb7bcdf8c",
      "value": 1
     }
    },
    "38ccdcaff71c4e10be16761856e700be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2702db773dda485487e0443e69c33aa4",
       "IPY_MODEL_5581a11b53e54b2fab584cd3f277ac03"
      ],
      "layout": "IPY_MODEL_84bb659b50644494b25209d5df54f495"
     }
    },
    "4aed2120237b48a099d2a8fdc6121949": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5581a11b53e54b2fab584cd3f277ac03": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_099a0a0fb2c34f578a460402c4a9695f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_70d729f1145a4b2d850d0f6f9643f4fc",
      "value": "170500096it [00:02, 76083211.55it/s]"
     }
    },
    "70d729f1145a4b2d850d0f6f9643f4fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84bb659b50644494b25209d5df54f495": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
